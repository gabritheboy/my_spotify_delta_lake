# ğŸ§ Spotify Delta Lake â€” Event-Driven Lakehouse (Databricks + AWS)

> A personal data engineering project simulating an **event-driven Delta Lakehouse architecture** built with **Databricks**, **AWS**, and the **Spotify Web API**.  
> Every time new Spotify data lands in the S3 bucket, a **Lambda function triggers** a Databricks **pipeline** that flattens, cleans, and enriches the data up to the **Silver Layer**.  
> The next step will be building a **Gold Layer** and an interactive **Streamlit dashboard** for personal music insights.

---

## ğŸš€ Project Goals

- Simulate a **real-world, event-driven architecture** using serverless components.
- Implement **incremental & idempotent pipelines** using Delta Lake **MERGE** operations.
- Build a **dimensional model** from unique Spotify IDs (artists, albums, tracks).
- Lay the foundation for a **Gold Layer** and **Streamlit** analytics app.

---

## ğŸ§© Architecture Overview

```mermaid
%% Compact horizontal flowchart
flowchart LR
  A[Spotify API] -->|Pull recently played| B[Lambda: get_user_recent_played_spotify_data]
  B -->|Write JSON| C[(S3: my-raw-spotify-data/user_recent_played/)]
  C -->|New file detected| D[Lambda: Databricks Trigger]
  D -->|Call Jobs API| E[Databricks Bronzeâ†’Silver Pipeline]
  E -->|Merge by timestamp| F[Silver.user_recent_played]
  F -->|Extract unique IDs| G[Parallel Tasks: Artists â€¢ Albums â€¢ Tracks]
  G -->|Fetch new metadata| H[(Bronze Dimensional Tables)]
  H -->|Autoloader| I[Silver Dimensional Tables]
  I --> J[(Future: Gold Layer + Streamlit Dashboard)]

```

# âš™ï¸ Components Breakdown

## 1. **Lambda â€” `get_user_recent_played_spotify_data`**
ğŸ“‚ **Path:** `/get_user_recent_played_spotify_data/`

Simulates an **event producer** calling the **Spotify API** endpoint for recently played tracks.

ğŸ“¤ **Output:**  
Writes raw **JSON files** to:

s3://my-raw-spotify-data/user_recent_played/



ğŸ•’ **Trigger:**  
In this prototype, ingestion is simulated through a **manual pull** or a **scheduled Lambda trigger**.

---

## 2. **Lambda â€” Databricks Trigger**

Monitors the **S3 bucket** for new objects.  
When a new file arrives, it **invokes the Databricks Jobs API** to start the **main ETL pipeline**.

ğŸ”„ This approach mimics a **real event-driven workflow** using **serverless compute**.

---

## 3. **Databricks Pipeline â€” Bronze â†’ Silver**

### ğŸŸ¤ Bronze Layer  
- Stores **raw Spotify API responses** as JSON.  
- Schema is **semi-structured** and may vary between calls.

### âšª Silver Layer  
- **Flattens nested JSONs** and converts them into a **structured, queryable table**:  


silver.user_recent_played


- Uses **MERGE ON timestamp** to ensure:  
âœ… Only **new records** are appended  
ğŸš« **Duplicates** are skipped  
âš¡ **Incremental and idempotent** writes

ğŸ§© The merge logic guarantees **data quality** without needing `dropDuplicates()` downstream.

---

## 4. **Databricks Pipeline â€” `[BRONZE/SILVER]_Dimensional_Data`**

âš™ï¸ **Trigger:** Runs automatically at the end of the main pipeline.  
ğŸ“¦ **Core Functions:**
- Extracts unique IDs (`artist_id`, `album_id`, `track_id`) from `silver.user_recent_played`.  
- Compares them with existing **dimensional tables** (if any).  
- Pulls new **metadata from the Spotify API** for unseen IDs.  

ğŸ“Š **Outputs:**
- Writes data into:  
- `bronze.artists`, `bronze.albums`, `bronze.tracks`  
- Uses **Autoloader** to transform and load curated data into:  
- `silver.artists`, `silver.albums`, `silver.tracks`

âš™ï¸ Each category runs as a **parallel task**, simulating a **distributed, event-driven process**.

---

## ğŸ§  Design Principles

- **Event-driven simulation** â†’ each new S3 object triggers an ETL job.  
- **Incremental & idempotent processing** â†’ Delta Lake `MERGE` ensures consistency.  
- **Layered architecture** â†’ Bronze â†’ Silver â†’ *(future)* Gold.  
- **Serverless orchestration** â†’ AWS Lambdas + Databricks Jobs API.  
- **Scalable design** â†’ easily extendable to Airflow, Step Functions, or EventBridge.  
- **Data lineage** â†’ from raw Spotify JSON to curated dimensional datasets.

---

## ğŸ§° Tech Stack

| **Component** | **Technology** | **Cloud** |
|----------------|----------------|------------|
| **Storage** | S3 (`my-raw-spotify-data`) | AWS |
| **Compute** | Databricks (Community Edition) | AWS |
| **Orchestration** | AWS Lambda + Databricks Jobs API | AWS |
| **Transformations** | PySpark + Delta Lake + Autoloader | â€” |
| **API Source** | Spotify Web API | â€” |
| **Future BI** | Streamlit | â€” |

---

## ğŸ“ Repository Structure

spotify-delta-lake/
â”‚
â”œâ”€â”€ get_user_recent_played_spotify_data/
â”‚   â””â”€â”€ lambda_function.py          # Pulls Spotify recently played data
â”‚
â”œâ”€â”€ databricks_notebooks/
â”‚   â”œâ”€â”€ bronze_to_silver_user_recent_played.ipynb
â”‚   â”œâ”€â”€ silver_dimensional_data.ipynb
â”‚   â””â”€â”€ utils/                      # Helper functions for flattening, merge, etc.
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ aws_env_variables.json
â”‚
â”œâ”€â”€ README.md                       # â† You are here
â””â”€â”€ requirements.txt


---

## ğŸ§ª Current Status

âœ… **Implemented**
- Raw â†’ Silver ingestion pipeline  
- Incremental `MERGE` logic  
- Dimensional data enrichment *(Artists, Albums, Tracks)*  
- Event-driven orchestration via AWS Lambda simulation

ğŸš§ **In Progress**
- Gold Layer modeling  
- Streamlit interactive dashboard  
- CI/CD setup *(GitHub Actions, Terraform, etc.)*

---

## ğŸ“Š Future Vision â€” Gold Layer & Dashboard

ğŸ¯ Next step: build a **Gold Layer** combining fact and dimensional tables to enable analytics such as:
- ğŸµ **Most played artists and genres**  
- â±ï¸ **Listening time per week or month**  
- ğŸ“ˆ **Audio feature distributions** *(danceability, energy, valence, etc.)*

ğŸ’» This layer will feed a **Streamlit dashboard**, potentially **containerized with Docker** for deployment.

---

## ğŸ’¡ Why This Project Matters

This project demonstrates how **modern data engineering principles** can be applied even to **personal datasets**.  
It showcases the ability to **design, automate, and orchestrate** an **end-to-end data lakehouse pipeline** using:
- **Real APIs**
- **Serverless compute**
- **Delta Lake best practices**

---

## ğŸ‘¤ Author

**Gabriele**  
ğŸ’¼ *Freelance Data Engineer & Entrepreneur*  
ğŸ“ *Building Data Products, Pipelines & Dashboards*  
ğŸ”— [LinkedIn] â€¢ [Portfolio] â€¢ [Spotify Developer Docs]

---

> ğŸ’­ *Would you like me to add a â€œQuick Start / Setupâ€ section (env vars, AWS setup, local simulation)?*  
That would make it perfect for recruiters or collaborators who want to **clone and run** your project.

